{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reviews and # .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrpLs6lUadWO"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import pandas as pd\n",
        "import urllib\n",
        "import bs4 as bs\n",
        "from bs4 import SoupStrainer\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request, urllib.error, urllib.parse\n",
        "import time\n",
        "from urllib.request import FancyURLopener  # This is library that helps us create the headless browser\n",
        "import random\n",
        "from random import choice  #This library helps pick a random item from a list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from urllib.request import FancyURLopener  # This is library that helps us create the headless browser\n",
        "import random\n",
        "from random import choice #This library helps pick a random item from a list\n",
        "\n",
        "# create user agents\n",
        "user_agents = [\n",
        "    'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
        "    'Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16',\n",
        "    'Mozilla/5.0 (Windows; U; Windows NT 6.1; rv:2.2) Gecko/20110201',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A',\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246'\n",
        "]\n",
        "\n",
        "# read csv file\n",
        "df = pd.read_csv(\"homeneeds.csv\", encoding = 'utf-8')\n",
        "\n",
        "# create product links\n",
        "link = []\n",
        "for i in df['product_link']:\n",
        "    index = i.find(\"ref=\")\n",
        "    i = i[:index]\n",
        "    link.append(\"https://www.amazon.com\" + i)\n",
        "\n",
        "# create review page link\n",
        "rating = []\n",
        "for i in df['rating_link']:\n",
        "    index = i.find(\"ref=\")\n",
        "    i = i[:index]\n",
        "    rating.append(\"https://www.amazon.com\" + i)    \n",
        "\n",
        "# define class for fancy url opener\n",
        "class MyOpener(FancyURLopener, object):\n",
        "    version = choice(user_agents)\n",
        "\n",
        "a = 1 # variable for denoting rank\n",
        "rank = [] # list for storing rank\n",
        "review = [] # list for storing review\n",
        "num_images = [] # list for storing number of images\n",
        "\n",
        "for i in link: # iterate over all product links\n",
        "    \n",
        "    try:\n",
        "        myopener = MyOpener()\n",
        "        page=myopener.open(i) # open page using fancy url\n",
        "        time.sleep(random.uniform(0,2))  #sleep time \n",
        "\n",
        "        html = page.read().decode('utf-8') # generate html for the page\n",
        "        soup = bs.BeautifulSoup(html) # create bs obejct for html source code\n",
        "\n",
        "        # get number of images\n",
        "        infotable = soup.find_all(\"li\", class_ = \"a-spacing-small item\")\n",
        "        num_images.append(len(infotable)) # will have length 100 for 100 products\n",
        "        \n",
        "    except:\n",
        "        num_images.append('no images')\n",
        "\n",
        "    # get link for review page\n",
        "    url = [] # list to capture link for review page\n",
        "    infotable = soup.find_all(\"a\", class_ = \"a-link-emphasis a-text-bold\")\n",
        "    if infotable: # if link for review page is found\n",
        "        for link in infotable:\n",
        "            url.append(\"https://www.amazon.com\" + link.get('href')) #create url for the review page\n",
        "        try:\n",
        "            # run the below code if review page opens\n",
        "            myopener = MyOpener()\n",
        "            page=myopener.open(url[0]) # open review page\n",
        "            time.sleep(random.uniform(0,2))  #sleep time \n",
        "            html = page.read().decode('utf-8')\n",
        "            soup = bs.BeautifulSoup(html)\n",
        "\n",
        "            # get top 10 reviews\n",
        "            infotable = soup.find_all(\"div\", class_ = \"a-row a-spacing-small review-data\")\n",
        "            for link in infotable:\n",
        "                review.append(link.getText()) # append each review\n",
        "                rank.append(a) #append rank for each review\n",
        "        except:\n",
        "            # run if review page does not open\n",
        "            rank.append(a)\n",
        "            review.append(\"no review\")\n",
        "    else: # if link for review page is not found\n",
        "        rank.append(a)\n",
        "        review.append(\"no review\")\n",
        "    \n",
        "    a = a+1 # go to next rank"
      ],
      "metadata": {
        "id": "70Zc2iIParFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writing the reviews back to a csv file\n",
        "import pandas as pd\n",
        "import csv\n",
        "df = pd.DataFrame(list(zip(rank, review)), \n",
        "            columns =['rank', 'review'])\n",
        "\n",
        "df.to_csv('/Users/ruthvik/Downloads/homeneeds_rating.csv', index = False)"
      ],
      "metadata": {
        "id": "WWKUctZTatL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writing the images back to a csv file\n",
        "df = pd.DataFrame(list(zip(num_images)), \n",
        "            columns =['num_images'])\n",
        "df.to_csv('/Users/ruthvik/Downloads/homeneeds_images.csv', index = False)"
      ],
      "metadata": {
        "id": "nFPufn5Oavaj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}